{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e647750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e90b0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ca5c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6797751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "872d37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rnn_class.brown import get_sentences_with_word2idx_limit_vocab as get_brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6094b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_2(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "\n",
    "def remove_punctuation_3(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "if sys.version.startswith('2'):\n",
    "    remove_punctuation = remove_punctuation_2\n",
    "else:\n",
    "    remove_punctuation = remove_punctuation_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc0d39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 2000\n",
    "  files = glob('../large_files/descricao_vagas.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cea6d9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function get_wiki at 0x0000018F6690A550>\n"
     ]
    }
   ],
   "source": [
    "print(get_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "294d5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(savedir):\n",
    "\n",
    "  sentences, word2idx = get_wiki() #get_brown()\n",
    "\n",
    "\n",
    "\n",
    "  vocab_size = len(word2idx)\n",
    "\n",
    "\n",
    "\n",
    "  window_size = 5\n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  num_negatives = 5 \n",
    "  epochs = 20\n",
    "  D = 50 \n",
    "\n",
    "\n",
    " \n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "\n",
    " \n",
    "  W = np.random.randn(vocab_size, D) \n",
    "  V = np.random.randn(D, vocab_size) \n",
    "\n",
    "\n",
    " \n",
    "  p_neg = get_negative_sampling_distribution(sentences, vocab_size)\n",
    "\n",
    "\n",
    "  \n",
    "  costs = []\n",
    "\n",
    "\n",
    "  \n",
    "  total_words = sum(len(sentence) for sentence in sentences)\n",
    "  print(\"total number of words in corpus:\", total_words)\n",
    "\n",
    "  \n",
    "  threshold = 1e-5\n",
    "  p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "\n",
    "\n",
    " \n",
    "  for epoch in range(epochs):\n",
    "    \n",
    "    np.random.shuffle(sentences)\n",
    "\n",
    "   \n",
    "    cost = 0\n",
    "    counter = 0\n",
    "    t0 = datetime.now()\n",
    "    for sentence in sentences:\n",
    "     \n",
    "      sentence = [w for w in sentence \\\n",
    "        if np.random.random() < (1 - p_drop[w])\n",
    "      ]\n",
    "      if len(sentence) < 2:\n",
    "        continue\n",
    "\n",
    "\n",
    "     \n",
    "      randomly_ordered_positions = np.random.choice(\n",
    "        len(sentence),\n",
    "        size=len(sentence),#np.random.randint(1, len(sentence) + 1),\n",
    "        replace=False,\n",
    "      )\n",
    "\n",
    "      \n",
    "      for pos in randomly_ordered_positions:\n",
    "       \n",
    "        word = sentence[pos]\n",
    "\n",
    "       \n",
    "        context_words = get_context(pos, sentence, window_size)\n",
    "        neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "        targets = np.array(context_words)\n",
    "\n",
    "        \n",
    "        c = sgd(word, targets, 1, learning_rate, W, V)\n",
    "        cost += c\n",
    "        c = sgd(neg_word, targets, 0, learning_rate, W, V)\n",
    "        cost += c\n",
    "\n",
    "      counter += 1\n",
    "      if counter % 100 == 0:\n",
    "        sys.stdout.write(\"processed %s / %s\\r\" % (counter, len(sentences)))\n",
    "        sys.stdout.flush()\n",
    "        # break\n",
    "\n",
    "\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "\n",
    "    \n",
    "    costs.append(cost)\n",
    "\n",
    "   \n",
    "    learning_rate -= learning_rate_delta\n",
    "\n",
    "\n",
    "  \n",
    "  plt.plot(costs)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    " \n",
    "  if not os.path.exists(savedir):\n",
    "    os.mkdir(savedir)\n",
    "\n",
    "  with open('%s/word2idx.json' % savedir, 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "  np.savez('%s/weights.npz' % savedir, W, V)\n",
    "\n",
    "  \n",
    "  return word2idx, W, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9be5e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences, vocab_size):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = np.zeros(vocab_size)\n",
    "  #word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          word_freq[word] += 1\n",
    "        \n",
    "  zero_count_idx = [i for i in range(len(word_freq)) if (word_freq[i]==0)]\n",
    "  print(\"zero_cound_idx\", zero_count_idx)\n",
    "  #sys.exit()\n",
    "\n",
    "  # smooth it\n",
    "  p_neg = word_freq**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b179191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    " \n",
    "\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      \n",
    "      context.append(ctx_word_idx)\n",
    "  return context\n",
    "\n",
    "\n",
    "def sgd(input_, targets, label, learning_rate, W, V):\n",
    "  \n",
    "  activation = W[input_].dot(V[:,targets])\n",
    "  prob = sigmoid(activation)\n",
    "\n",
    " \n",
    "  gV = np.outer(W[input_], prob - label) # D x N\n",
    "  gW = np.sum((prob - label)*V[:,targets], axis=1) # D\n",
    "\n",
    "  V[:,targets] -= learning_rate*gV # D x N\n",
    "  W[input_] -= learning_rate*gW # D\n",
    "\n",
    "  \n",
    "  cost = label * np.log(prob + 1e-10) + (1 - label) * np.log(1 - prob + 1e-10)\n",
    "  return cost.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ee7ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(savedir):\n",
    "  with open('%s/word2idx.json' % savedir) as f:\n",
    "    word2idx = json.load(f)\n",
    "  npz = np.load('%s/weights.npz' % savedir)\n",
    "  W = npz['arr_0']\n",
    "  V = npz['arr_1']\n",
    "  return word2idx, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a1ade1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
    "  V, D = W.shape\n",
    "\n",
    "  \n",
    "  print(\"testing: %s - %s = %s - %s\" % (pos1, neg1, pos2, neg2))\n",
    "  for w in (pos1, neg1, pos2, neg2):\n",
    "    if w not in word2idx:\n",
    "      print(\"Sorry, %s not in word2idx\" % w)\n",
    "      return\n",
    "\n",
    "  p1 = W[word2idx[pos1]]\n",
    "  n1 = W[word2idx[neg1]]\n",
    "  p2 = W[word2idx[pos2]]\n",
    "  n2 = W[word2idx[neg2]]\n",
    "\n",
    "  vec = p1 - n1 + n2\n",
    "\n",
    "  distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
    "  idx = distances.argsort()[:10]\n",
    "\n",
    " \n",
    "  best_idx = -1\n",
    "  keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n",
    " \n",
    "  for i in idx:\n",
    "    if i not in keep_out:\n",
    "      best_idx = i\n",
    "      break\n",
    "  \n",
    "\n",
    "  print(\"got: %s - %s = %s - %s\" % (pos1, neg1, idx2word[best_idx], neg2))\n",
    "  print(\"closest 10:\")\n",
    "  for i in idx:\n",
    "    print(idx2word[i], distances[i])\n",
    "\n",
    "  print(\"dist to %s:\" % pos2, cos_dist(p2, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce30dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(word2idx, W, V):\n",
    " \n",
    "\n",
    "  idx2word = {i:w for w, i in word2idx.items()}\n",
    "\n",
    "  for We in (W, (W + V.T) / 2):\n",
    "    print(\"**********\")\n",
    "\n",
    "    analogy('machine', 'leraning', 'big', 'data', word2idx, idx2word, We)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b5ebf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n",
      "zero_cound_idx [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas.miranda\\AppData\\Local\\Temp\\ipykernel_11120\\2352422927.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p_neg = p_neg / p_neg.sum()\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word2idx, W, V \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw2v_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_model(word2idx, W, V)\n",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(savedir)\u001b[0m\n\u001b[0;32m     24\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(vocab_size, D) \n\u001b[0;32m     25\u001b[0m V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(D, vocab_size) \n\u001b[1;32m---> 29\u001b[0m p_neg \u001b[38;5;241m=\u001b[39m \u001b[43mget_negative_sampling_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m costs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m total_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences)\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mget_negative_sampling_distribution\u001b[1;34m(sentences, vocab_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# normalize it\u001b[39;00m\n\u001b[0;32m     21\u001b[0m p_neg \u001b[38;5;241m=\u001b[39m p_neg \u001b[38;5;241m/\u001b[39m p_neg\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mall(p_neg \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p_neg\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word2idx, W, V = train_model('w2v_model')\n",
    "test_model(word2idx, W, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093aeabd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
